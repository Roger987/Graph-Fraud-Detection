{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234355/234355 [00:01<00:00, 121538.41it/s]\n"
     ]
    }
   ],
   "source": [
    "classes_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_classes.csv\"\n",
    "edges_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_edgelist.csv\"\n",
    "features_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_features.csv\"\n",
    "\n",
    "classes = pd.read_csv(classes_path)\n",
    "edges = pd.read_csv(edges_path)\n",
    "feat_cols = ['txId', 'time_step'] + [f'trans_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]\n",
    "feats = pd.read_csv(features_path, header=None, names=feat_cols)\n",
    "\n",
    "# Preprocess the classes DataFrame\n",
    "classes.columns = ['txId', 'label']\n",
    "df = classes.set_index('txId').join(feats.set_index('txId'))\n",
    "\n",
    "# Create a mapping for all nodes\n",
    "all_nodes_dict = {tx_id: i for i, tx_id in enumerate(classes['txId'])}\n",
    "\n",
    "# Create edges list with all nodes\n",
    "edges_list = [\n",
    "    (all_nodes_dict[edges['txId1'][i]], all_nodes_dict[edges['txId2'][i]])\n",
    "    for i in tqdm(range(len(edges)))\n",
    "    if edges['txId1'][i] in all_nodes_dict and edges['txId2'][i] in all_nodes_dict\n",
    "]\n",
    "edge_index = torch.tensor(edges_list, dtype=torch.long).T\n",
    "\n",
    "# Convert node features and labels    Parameter to (V) choose features used\n",
    "#node_features = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float)\n",
    "time_step = torch.tensor(df['time_step'].values, dtype=torch.float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.iloc[:, 2:].values)  # Exclude txId and time_step\n",
    "\n",
    "# Combine time_step back with scaled features\n",
    "node_features = torch.cat((time_step.unsqueeze(1), torch.tensor(scaled_features, dtype=torch.float)), dim=1)\n",
    "\n",
    "label_mapping = {'1': 0, '2': 1, 'unknown': -1} \n",
    "labels = torch.tensor(classes['label'].map(label_mapping).values, dtype=torch.long)\n",
    "\n",
    "# Create graph data object\n",
    "data = Data(x=node_features, edge_index=edge_index, y=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_column_index = 0 \n",
    "time_step = data.x[:, time_step_column_index]\n",
    "\n",
    "train_mask = (time_step >= 1) & (time_step <= 34)\n",
    "test_mask = (time_step >= 35) & (time_step <= 49)\n",
    "\n",
    "train_mask = train_mask.clone().detach().to(torch.bool)\n",
    "test_mask = test_mask.clone().detach().to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcn): GCNConv(166, 128)\n",
      "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "2\n",
      "Epoch   0 | Loss: 0.75 | Acc: 27.19%\n",
      "Epoch  10 | Loss: 0.41 | Acc: 87.78%\n",
      "Epoch  20 | Loss: 0.29 | Acc: 86.39%\n",
      "Epoch  30 | Loss: 0.24 | Acc: 86.52%\n",
      "Epoch  40 | Loss: 0.20 | Acc: 89.00%\n",
      "Epoch  50 | Loss: 0.17 | Acc: 90.60%\n",
      "Epoch  60 | Loss: 0.15 | Acc: 91.41%\n",
      "Epoch  70 | Loss: 0.13 | Acc: 93.13%\n",
      "Epoch  80 | Loss: 0.13 | Acc: 92.17%\n",
      "Epoch  90 | Loss: 0.12 | Acc: 94.33%\n",
      "Epoch 100 | Loss: 0.11 | Acc: 94.98%\n",
      "Epoch 110 | Loss: 0.11 | Acc: 94.92%\n",
      "Epoch 120 | Loss: 0.10 | Acc: 95.03%\n",
      "Epoch 130 | Loss: 0.10 | Acc: 93.80%\n",
      "Epoch 140 | Loss: 0.10 | Acc: 96.40%\n",
      "Epoch 150 | Loss: 0.12 | Acc: 91.84%\n",
      "Epoch 160 | Loss: 0.10 | Acc: 95.08%\n",
      "Epoch 170 | Loss: 0.09 | Acc: 95.95%\n",
      "Epoch 180 | Loss: 0.09 | Acc: 95.98%\n",
      "Epoch 190 | Loss: 0.08 | Acc: 95.85%\n",
      "Epoch 200 | Loss: 0.08 | Acc: 96.16%\n",
      "Epoch 210 | Loss: 0.08 | Acc: 96.08%\n",
      "Epoch 220 | Loss: 0.08 | Acc: 95.15%\n",
      "Epoch 230 | Loss: 0.08 | Acc: 94.57%\n",
      "Epoch 240 | Loss: 0.08 | Acc: 96.22%\n",
      "Epoch 250 | Loss: 0.08 | Acc: 96.24%\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "# embeddings length = 128\n",
    "# Leaky ReLU\n",
    "# lr = 0.02\n",
    "# weight_decay = 0.001\n",
    "# epochs = 251\n",
    "# Precision node embeddings (GCN): 0.8458498023715415\n",
    "# Recall node embeddings (GCN): 0.19759926131117267\n",
    "# F1 node embeddings (GCN): 0.3203592814371258\n",
    "# Cross-validated F1 Score node embeddings (GCN): 0.500450874236994\n",
    "# Precision embeddings + all features: 0.9718670076726342\n",
    "# Recall embeddings + all features: 0.7017543859649122\n",
    "# F1 embeddings + all features: 0.8150134048257373\n",
    "# Cross-validated F1 Score embeddings + all features: 0.6736564758872677\n",
    "\n",
    "embeddings_length = 128\n",
    "lr = 0.02\n",
    "weight_decay = 0.001\n",
    "epochs = 251\n",
    "\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(num_features, embeddings_length)  # GCNConv layer, 3 output channels\n",
    "        self.out = Linear(embeddings_length, num_classes)    # Linear layer for classification output\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        h = torch.nn.functional.leaky_relu(self.gcn(x, edge_index))   # Apply GCN and ReLU\n",
    "        z = self.out(h)                      # Output layer\n",
    "        return h, z\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_features = data.x.shape[1]  # Number of features (columns in x)\n",
    "num_classes = 2  # Number of classes (2 in this case)\n",
    "model = GCN(num_features, num_classes)\n",
    "print(model)\n",
    "print(num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "\n",
    "valid_labels = classes['label'].map(label_mapping)\n",
    "valid_labels = valid_labels[valid_labels != -1]  # Exclude 'unknown'\n",
    "\n",
    "# Compute class weights only for valid labels\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),  # Include only valid classes\n",
    "    y=valid_labels\n",
    ")\n",
    "\n",
    "# Convert to a PyTorch tensor for use in the loss function\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Accuracy calculation function\n",
    "def accuracy(pred_y, y):\n",
    "    return (pred_y == y).sum() / len(y)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    h, z = model(data.x, data.edge_index)  # h: embeddings, z: logits\n",
    "\n",
    "    # Exclude unlabeled nodes from the loss calculation\n",
    "    mask = data.y[train_mask] != -1  \n",
    "    loss = criterion(z[train_mask][mask], data.y[train_mask][mask]) # Compute loss\n",
    "    \n",
    "    loss.backward()                         # Backpropagate\n",
    "    optimizer.step()                        # Update model parameters\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = accuracy(z[train_mask].argmax(dim=1)[mask], data.y[train_mask][mask])  # Calculate accuracy\n",
    "        print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings, _ = model(data.x, data.edge_index)  # h: embeddings\n",
    "\n",
    "# Ensure alignment of txId and labels with embeddings\n",
    "# The order in `data.x` corresponds to `classes['txId']` due to how `all_nodes_dict` was built\n",
    "aligned_df = pd.DataFrame({\n",
    "    'txId': classes['txId'],  # Use the original node order\n",
    "    'time_step': time_step.cpu().numpy(),  # Extract time_step from the GCN input\n",
    "    'label': classes['label']  # Use the original labels\n",
    "})\n",
    "\n",
    "# Add embeddings\n",
    "embeddings_df = pd.concat([aligned_df, pd.DataFrame(embeddings.cpu().numpy())], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "embeddings_df.to_csv('embeddings_gcn.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
