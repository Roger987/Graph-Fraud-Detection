{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, LeakyReLU, Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234355/234355 [00:02<00:00, 102818.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "classes_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_classes.csv\"\n",
    "edges_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_edgelist.csv\"\n",
    "features_path = \"../elliptic_bitcoin_dataset/modified_elliptic_txs_features.csv\"\n",
    "\n",
    "classes = pd.read_csv(classes_path)\n",
    "edges = pd.read_csv(edges_path)\n",
    "feat_cols = ['txId', 'time_step'] + [f'trans_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]\n",
    "feats = pd.read_csv(features_path, header=None, names=feat_cols)\n",
    "\n",
    "# Preprocess the classes DataFrame\n",
    "classes.columns = ['txId', 'label']\n",
    "df = classes.set_index('txId').join(feats.set_index('txId'))\n",
    "\n",
    "# Create a mapping for all nodes\n",
    "all_nodes_dict = {tx_id: i for i, tx_id in enumerate(classes['txId'])}\n",
    "\n",
    "# Create edges list with all nodes\n",
    "edges_list = [\n",
    "    (all_nodes_dict[edges['txId1'][i]], all_nodes_dict[edges['txId2'][i]])\n",
    "    for i in tqdm(range(len(edges)))\n",
    "    if edges['txId1'][i] in all_nodes_dict and edges['txId2'][i] in all_nodes_dict\n",
    "]\n",
    "edge_index = torch.tensor(edges_list, dtype=torch.long).T\n",
    "\n",
    "# Convert node features and labels    Parameter to (V) choose features used\n",
    "#node_features = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float)\n",
    "time_step = torch.tensor(df['time_step'].values, dtype=torch.float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.iloc[:, 2:].values)  # Exclude txId and time_step\n",
    "\n",
    "# Combine time_step back with scaled features\n",
    "node_features = torch.cat((time_step.unsqueeze(1), torch.tensor(scaled_features, dtype=torch.float)), dim=1)\n",
    "\n",
    "label_mapping = {'1': 0, '2': 1, 'unknown': -1} \n",
    "labels = torch.tensor(classes['label'].map(label_mapping).values, dtype=torch.long)\n",
    "\n",
    "# Create graph data object\n",
    "data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "data = data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_column_index = 0 \n",
    "time_step = data.x[:, time_step_column_index]\n",
    "\n",
    "train_mask = (time_step >= 1) & (time_step <= 34)\n",
    "test_mask = (time_step >= 35) & (time_step <= 49)\n",
    "\n",
    "train_mask = train_mask.clone().detach().to(torch.bool)\n",
    "test_mask = test_mask.clone().detach().to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=166, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (gin): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=166, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  ))\n",
      "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "2\n",
      "Epoch   0 | Loss: 0.79 | Acc: 15.91%\n",
      "Epoch  10 | Loss: 0.53 | Acc: 76.89%\n",
      "Epoch  20 | Loss: 0.33 | Acc: 83.11%\n",
      "Epoch  30 | Loss: 0.29 | Acc: 83.87%\n",
      "Epoch  40 | Loss: 0.25 | Acc: 88.42%\n",
      "Epoch  50 | Loss: 0.23 | Acc: 86.48%\n",
      "Epoch  60 | Loss: 0.20 | Acc: 90.50%\n",
      "Epoch  70 | Loss: 0.18 | Acc: 92.76%\n",
      "Epoch  80 | Loss: 0.21 | Acc: 86.41%\n",
      "Epoch  90 | Loss: 0.20 | Acc: 85.81%\n",
      "Epoch 100 | Loss: 0.20 | Acc: 89.42%\n",
      "Epoch 110 | Loss: 0.16 | Acc: 91.73%\n",
      "Epoch 120 | Loss: 0.14 | Acc: 94.19%\n",
      "Epoch 130 | Loss: 0.15 | Acc: 96.10%\n",
      "Epoch 140 | Loss: 0.18 | Acc: 87.88%\n",
      "Epoch 150 | Loss: 0.14 | Acc: 92.96%\n",
      "Epoch 160 | Loss: 0.12 | Acc: 94.69%\n",
      "Epoch 170 | Loss: 0.11 | Acc: 95.45%\n",
      "Epoch 180 | Loss: 0.12 | Acc: 95.23%\n",
      "Epoch 190 | Loss: 0.10 | Acc: 96.54%\n",
      "Epoch 200 | Loss: 0.22 | Acc: 81.43%\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "# embeddings length = 128\n",
    "# Leaky ReLU\n",
    "# lr = 0.02\n",
    "# weight_decay = 0.001\n",
    "# epochs = 251\n",
    "\n",
    "embeddings_length = 128\n",
    "lr = 0.02\n",
    "weight_decay = 0.001\n",
    "epochs = 201\n",
    "\n",
    "\n",
    "# Define model\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp = Sequential(\n",
    "            Linear(num_features, embeddings_length*2),  \n",
    "            LeakyReLU(negative_slope=0.2), \n",
    "            Linear(embeddings_length*2, embeddings_length)  \n",
    "        )\n",
    "        self.gin = GINConv(self.mlp)\n",
    "        self.out = Linear(embeddings_length, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        h = torch.nn.functional.leaky_relu(self.gin(x, edge_index))\n",
    "        z = self.out(h)\n",
    "        \n",
    "        return h, z\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_features = data.x.shape[1]  # Number of features (columns in x)\n",
    "num_classes = 2  # Number of classes (2 in this case)\n",
    "model = GIN(num_features, num_classes)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "\n",
    "valid_labels = classes['label'].map(label_mapping)\n",
    "valid_labels = valid_labels[valid_labels != -1]  # Exclude 'unknown'\n",
    "\n",
    "# Compute class weights only for valid labels\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),  # Include only valid classes\n",
    "    y=valid_labels\n",
    ")\n",
    "\n",
    "# Convert to a PyTorch tensor for use in the loss function\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor).to(device)\n",
    "\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Accuracy calculation function\n",
    "def accuracy(pred_y, y):\n",
    "    return (pred_y == y).sum() / len(y)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    h, z = model(data.x, data.edge_index)  # h: embeddings, z: logits\n",
    "\n",
    "    # Exclude unlabeled nodes from the loss calculation\n",
    "    mask = data.y[train_mask] != -1  \n",
    "    loss = criterion(z[train_mask][mask], data.y[train_mask][mask]) # Compute loss\n",
    "    \n",
    "    loss.backward()                         # Backpropagate\n",
    "    optimizer.step()                        # Update model parameters\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = accuracy(z[train_mask].argmax(dim=1)[mask], data.y[train_mask][mask])  # Calculate accuracy\n",
    "        print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings, _ = model(data.x, data.edge_index)  # h: embeddings\n",
    "\n",
    "# Ensure alignment of txId and labels with embeddings\n",
    "# The order in `data.x` corresponds to `classes['txId']` due to how `all_nodes_dict` was built\n",
    "aligned_df = pd.DataFrame({\n",
    "    'txId': classes['txId'],  # Use the original node order\n",
    "    'time_step': time_step.cpu().numpy(),  # Extract time_step from the GCN input\n",
    "    'label': classes['label']  # Use the original labels\n",
    "})\n",
    "\n",
    "# Add embeddings\n",
    "embeddings_df = pd.concat([aligned_df, pd.DataFrame(embeddings.cpu().numpy())], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "embeddings_df.to_csv('../data/embeddings_gin.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
